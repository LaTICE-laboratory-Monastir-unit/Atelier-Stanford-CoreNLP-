{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* Tokenization of raw text is a standard pre-processing step for many NLP tasks.\n",
    "\n",
    "* For English, tokenization usually involves punctuation splitting and separation of some affixes like possessives.\n",
    "\n",
    "* Other languages require more extensive token pre-processing, which is usually called segmentation.\n",
    "\n",
    "* The Stanford Word Segmenter currently supports Arabic and Chinese. (The Stanford Tokenizer can be used for English, French, and Spanish.) The provided segmentation schemes have been found to work well for a variety of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "* The system requires Java 1.8+ to be installed.\n",
    "* stanford-segmenter can be found here https://nlp.stanford.edu/software/segmenter.html\n",
    "* NTLK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Arabic Stanford Word Segmenter paper\n",
    "* http://nlp.stanford.edu/pubs/monroe-green-manning-acls2014.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Set paths to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "java_path = \"C:\\\\Program Files\\\\Java\\\\jdk1.8.0_131\\\\bin\\\\java.exe\"\n",
    "slf4j_path ='C:\\\\stanford-segmenter\\\\slf4j-api.jar'\n",
    "stanford_models_paths = 'C:\\\\stanford-segmenter\\\\data'\n",
    "classpath = 'C:\\\\stanford-segmenter\\\\slf4j-api.jar;C:\\\\stanford-segmenter\\\\stanford-segmenter.jar'\n",
    "nltk.internals.config_java(java_path)\n",
    "os.environ['JAVAHOME'] = java_path\n",
    "os.environ['SLF4J'] =slf4j_path\n",
    "os.environ['STANFORD_MODELS'] =stanford_models_paths\n",
    "os.environ['CLASSPATH'] = classpath"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Using the segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenizer = StanfordSegmenter()\n",
    "tokenizer.default_config('ar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Segmente a sentence  (list of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentence = ['من!', 'أنت', 'وقالها']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentence_tekonized = tokenizer.segment(sentence)\n",
    "print(sentence_tekonized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Segmente a list of sentences  (list of list of words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentences = [['من!', 'أنت', 'وقالها'],\n",
    "             ['من!', 'أنت', 'وقالها']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sentences_tekonized = tokenizer.segment_sents(sentences)\n",
    "print(sentences_tekonized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Segmente a text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def split(string):\n",
    "    return string.split()\n",
    "def tokenize_file(input_file,output_file):\n",
    "    with open(output_file, 'w',encoding='utf8') as new_file:\n",
    "        with open(input_file, 'r',encoding='utf8') as f:\n",
    "            content = f.readlines()\n",
    "            content = [x.strip() for x in content] \n",
    "        print('Number of lines : ',len(content))\n",
    "        lines = tokenizer.segment_sents(list(map(split, content))).splitlines()\n",
    "        new_file.write('\\n'.join(lines))\n",
    "    print('Done ,see :',output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "tokenize_file(input_file='./texts.txt',output_file='./texts_tekonized.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#http://nlp.stanford.edu:8080/parser/index.jsp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
